{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03bef15",
   "metadata": {},
   "source": [
    "# 스테이블 디퓨전으로 이미지 생성 서비스 만들기\n",
    "- 간단한 스케치를 기반으로 이미지 생성\n",
    "- 스케치가 되어 있는 이미지를 업로드해서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80155059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17251b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import IO\n",
    "import gradio as gr\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f4ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.40.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 512\n",
    "HEIGHT = 512\n",
    "\n",
    "with gr.Blocks() as app :\n",
    "    gr.Markdown(\"## 프롬프트 입력\")\n",
    "    with gr.Row() :\n",
    "        prompt = gr.Textbox(label = 'Prompt')\n",
    "    with gr.Row() :\n",
    "        n_prompt = gr.Textbox(label = \"negative prompt\")\n",
    "        \n",
    "    gr.Markdown('## 스케치 to 이미지 생성')\n",
    "    with gr.Row() :\n",
    "        with gr.Column() :\n",
    "            with gr.Tab(\"Canvas\") :\n",
    "                with gr.Row() :\n",
    "                    canvas = gr.Image(\n",
    "                        label = 'Draw',\n",
    "                        source = 'canvas',\n",
    "                        image_mode = 'RGB',\n",
    "                        tool = 'color-sketch',\n",
    "                        interactive = True,\n",
    "                        width = WIDTH,\n",
    "                        height = HEIGHT,\n",
    "                        shape = (WIDTH, HEIGHT),\n",
    "                        brush_radius = 20,\n",
    "                        type = 'pil'\n",
    "                    )\n",
    "                with gr.Row() :\n",
    "                    canvas_run_btn = gr.Button(value = 'Generate')\n",
    "\n",
    "            with gr.Tab(\"File\") :\n",
    "                with gr.Row() :\n",
    "                    file = gr.Image(\n",
    "                        label = 'Upload',\n",
    "                        source = 'upload',\n",
    "                        image_mode = 'RGB',\n",
    "                        tool = 'color-sketch',\n",
    "                        interactive = True,\n",
    "                        width = WIDTH,\n",
    "                        height = HEIGHT,\n",
    "                        shape = (WIDTH, HEIGHT),\n",
    "                        type = 'pil'\n",
    "                    )\n",
    "                with gr.Row() :\n",
    "                    file_run_btn = gr.Button(value = 'Generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8864406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a8b6f5f98efdf2363c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.launch(inline = False, share = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee08fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d14dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1f24b02",
   "metadata": {},
   "source": [
    "## 모델 다운로드 UI 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144e3de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.40.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as app :\n",
    "    gr.Markdown('## 모델 다운로드')\n",
    "    with gr.Row() :\n",
    "        model_url = gr.Textbox(label = '모델 URL', placeholder = 'http://civitai.com/')\n",
    "        download_model_btn = gr.Button(value = '모델 다운로드')\n",
    "        \n",
    "    with gr.Row() :\n",
    "        model_file = gr.File(label = '모델 File')\n",
    "        \n",
    "    download_model_btn.click(\n",
    "        download_model,\n",
    "        [model_url],\n",
    "        [model_file]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5e25a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "Running on public URL: https://f0b1d07ba918cead6e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models/disneyPixarCartoon_v10.safetensors: 100%|██████████████████████████████████| 3.95G/3.95G [11:24<00:00, 6.20MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] file downloaded : models/disneyPixarCartoon_v10.safetensors\n"
     ]
    }
   ],
   "source": [
    "app.launch(inline = False, share = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2272be29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda1754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f0c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9ff792",
   "metadata": {},
   "source": [
    "## 모델 다운로드 기능 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b12d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 디렉토리 안에 있는 파일명을 리스트로 만들어주는 라이브러리\n",
    "import glob\n",
    "\n",
    "# 전역 변수로 모델 경로와 파일명을 저장\n",
    "MODEL_PATH = None\n",
    "\n",
    "# URL로부터 파일 다운로드하는 함수\n",
    "def download_from_url(url, file_path, chunk_size = 1024) :\n",
    "    try :\n",
    "        resp = requests.get(url, stream = True)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f'[error] {e}')\n",
    "        raise e\n",
    "        \n",
    "    total = int(resp.headers.get('content-length', 0)) # 파일 크기 추출\n",
    "    with open(file_path, 'wb') as file, tqdm(desc = file_path, total = total, unit = 'iB', unit_scale = True,\n",
    "                                            unit_divisor = 1024) as bar:\n",
    "        for data in resp.iter_content(chunk_size = chunk_size) :\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "# 모델을 다운로드하고 경로를 기억하는 함수\n",
    "def download_model(url: str) -> str :\n",
    "    global MODEL_PATH #전역변수를 사용해서 경로를 기억\n",
    "    \n",
    "    model_id = url.replace('https://civitai.com/models/', \"\").split('/')[0]\n",
    "    \n",
    "    try :\n",
    "        response = requests.get(f'https://civitai.com/api/v1/models/{model_id}', timeout = 6000)\n",
    "    except Exception as e :\n",
    "        print(f'Error : {e}')\n",
    "        raise e\n",
    "        \n",
    "    download_url = response.json()['modelVersions'][0]['downloadUrl']\n",
    "    filename = response.json()['modelVersions'][0]['files'][0]['name']\n",
    "    \n",
    "    file_path = f'models/{filename}'\n",
    "    if os.path.exists(file_path) :\n",
    "        print(f'[info] File already exists : {file_path}')\n",
    "        MODEL_PATH = file_path\n",
    "        return file_path\n",
    "    \n",
    "    os.makedirs('models', exist_ok = True)\n",
    "    download_from_url(download_url, file_path)\n",
    "    print(f'[info] file downloaded : {file_path}')\n",
    "    \n",
    "    # 모델 경로 기억\n",
    "    MODEL_PATH = file_path\n",
    "    return file_path\n",
    "\n",
    "# ./models 폴더에서 가장 최근에 수정된 모델 파일 찾기\n",
    "def find_latest_model_in_directory(directory) :\n",
    "    model_files = glob.glob(f'{directory}/*.safetensors')\n",
    "    if not model_files :\n",
    "        return None\n",
    "    \n",
    "    # 가장 최근에 수정된 모델 파일 선택\n",
    "    latest_model = max(model_files, key = os.path.getmtime)\n",
    "    return latest_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebab3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c14a53fd",
   "metadata": {},
   "source": [
    "## 다운로드한 모델 불러와서 초기화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aad44bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pipeline() :\n",
    "    global MODEL_PATH\n",
    "    \n",
    "    if MODEL_PATH is None :\n",
    "        MODEL_PATH = find_latest_model_in_directory('./models/')\n",
    "    if MODEL_PATH is None :\n",
    "        return \"Error: No model found in ./models\"\n",
    "    \n",
    "    global PIPELINE\n",
    "    \n",
    "    try :\n",
    "        PIPELINE = StableDiffusionImg2ImgPipeline.from_single_file(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype = torch.float16,\n",
    "            variant = 'fp16',\n",
    "            use_safetensors = True,\n",
    "        ).to('cpu')\n",
    "        print('[info] initiallized pipeline')\n",
    "        return 'Model Loaded!'\n",
    "    except Exception as e :\n",
    "        print(f'[error] {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae42df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.40.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as app :\n",
    "    gr.Markdown('## 모델 불러오기')\n",
    "    with gr.Row() :\n",
    "        load_model_btn = gr.Button(value = '모델 불러오기')\n",
    "    with gr.Row() :\n",
    "        is_model_check = gr.Textbox(label = 'Model Load Check', value = 'model not loaded')\n",
    "    \n",
    "    load_model_btn.click(\n",
    "        init_pipeline,\n",
    "        None,\n",
    "        [is_model_check]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e1cfdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://2023fa78bcd092a206.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|█████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 95522.45it/s]\n",
      "Loading pipeline components...:   0%|                                                             | 0/6 [00:00<?, ?it/s]/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint were not used when initializing CLIPTextModel: \n",
      " ['text_model.embeddings.position_ids']\n",
      "Loading pipeline components...: 100%|█████████████████████████████████████████████████████| 6/6 [00:29<00:00,  4.89s/it]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] initiallized pipeline\n"
     ]
    }
   ],
   "source": [
    "app.queue().launch(inline = False, share = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebf55cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "app.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d63ce",
   "metadata": {},
   "source": [
    "## 스케치 to 이미지 생성기능 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "308edff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch_to_image(sketch, prompt, negative_prompt) :\n",
    "    global PIPELINE\n",
    "    if PIPELINE is None :\n",
    "        return \"error! pipeline is not initialized\"\n",
    "    \n",
    "    prompt = [prompt]\n",
    "    negative_prompt = [negative_prompt]\n",
    "    \n",
    "    images = [sketch] * len(prompt)\n",
    "    \n",
    "    try :\n",
    "        # 이미지 생성\n",
    "        result = PIPELINE(\n",
    "            image = images,\n",
    "            prompt = prompt,\n",
    "            negative_prompt = negative_prompt,\n",
    "            height = height,\n",
    "            width = width,\n",
    "            num_images_per_prompt = 4,\n",
    "            num_inference_steps = 20,\n",
    "            strength = 0.7\n",
    "        ).images\n",
    "        \n",
    "    except Exception as e :\n",
    "        print(e)\n",
    "        \n",
    "#     # gpu 메모리 캐시 비우기\n",
    "#     with torch.cuda.device('cuda') :\n",
    "#         torch.cuda.empty_cache()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2cf7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2229cff6",
   "metadata": {},
   "source": [
    "# 이미지 생성 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8fae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import IO\n",
    "import gradio as gr\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import torch\n",
    "# 디렉토리 안에 있는 파일명을 리스트로 만들어주는 라이브러리\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b22831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케치하고 이미지 업로드하는 ui\n",
    "WIDTH = 512\n",
    "HEIGHT = 512\n",
    "\n",
    "# 전역 변수로 모델 경로와 파일명을 저장\n",
    "MODEL_PATH = None\n",
    "PIPELINE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3046b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL로부터 파일 다운로드하는 함수\n",
    "def download_from_url(url, file_path, chunk_size = 1024) :\n",
    "    try :\n",
    "        resp = requests.get(url, stream = True)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f'[error] {e}')\n",
    "        raise e\n",
    "        \n",
    "    total = int(resp.headers.get('content-length', 0)) # 파일 크기 추출\n",
    "    with open(file_path, 'wb') as file, tqdm(desc = file_path, total = total, unit = 'iB', unit_scale = True,\n",
    "                                            unit_divisor = 1024) as bar:\n",
    "        for data in resp.iter_content(chunk_size = chunk_size) :\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "# 모델을 다운로드하고 경로를 기억하는 함수\n",
    "def download_model(url: str) -> str :\n",
    "    global MODEL_PATH #전역변수를 사용해서 경로를 기억\n",
    "    \n",
    "    model_id = url.replace('https://civitai.com/models/', \"\").split('/')[0]\n",
    "    \n",
    "    try :\n",
    "        response = requests.get(f'https://civitai.com/api/v1/models/{model_id}', timeout = 6000)\n",
    "    except Exception as e :\n",
    "        print(f'Error : {e}')\n",
    "        raise e\n",
    "        \n",
    "    download_url = response.json()['modelVersions'][0]['downloadUrl']\n",
    "    filename = response.json()['modelVersions'][0]['files'][0]['name']\n",
    "    \n",
    "    file_path = f'models/{filename}'\n",
    "    if os.path.exists(file_path) :\n",
    "        print(f'[info] File already exists : {file_path}')\n",
    "        MODEL_PATH = file_path\n",
    "        return file_path\n",
    "    \n",
    "    os.makedirs('models', exist_ok = True)\n",
    "    download_from_url(download_url, file_path)\n",
    "    print(f'[info] file downloaded : {file_path}')\n",
    "    \n",
    "    # 모델 경로 기억\n",
    "    MODEL_PATH = file_path\n",
    "    return file_path\n",
    "\n",
    "# ./models 폴더에서 가장 최근에 수정된 모델 파일 찾기\n",
    "def find_latest_model_in_directory(directory) :\n",
    "    model_files = glob.glob(f'{directory}/*.safetensors')\n",
    "    if not model_files :\n",
    "        return None\n",
    "    \n",
    "    # 가장 최근에 수정된 모델 파일 선택\n",
    "    latest_model = max(model_files, key = os.path.getmtime)\n",
    "    return latest_model\n",
    "\n",
    "# 다운로드한 모델 불러와서 초기화하기\n",
    "def init_pipeline() :\n",
    "    global MODEL_PATH\n",
    "    \n",
    "    if MODEL_PATH is None :\n",
    "        MODEL_PATH = find_latest_model_in_directory('./models/')\n",
    "    if MODEL_PATH is None :\n",
    "        return \"Error: No model found in ./models\"\n",
    "    \n",
    "    global PIPELINE\n",
    "    \n",
    "    try :\n",
    "        PIPELINE = StableDiffusionImg2ImgPipeline.from_single_file(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype = torch.float32, #cpu와 호환되도록 float34로 변경\n",
    "            variant = 'fp32',\n",
    "            use_safetensors = True,\n",
    "        ).to('cpu')\n",
    "        print('[info] initiallized pipeline')\n",
    "        return 'Model Loaded!'\n",
    "    except Exception as e :\n",
    "        print(f'[error] {e}')\n",
    "\n",
    "# 스케치 to 이미지 생성기능 구현\n",
    "def sketch_to_image(sketch, prompt, negative_prompt) :\n",
    "    global PIPELINE\n",
    "    if PIPELINE is None :\n",
    "        return \"error! pipeline is not initialized\"\n",
    "    \n",
    "    prompt = [prompt]\n",
    "    negative_prompt = [negative_prompt]\n",
    "    \n",
    "    images = [sketch] * len(prompt)\n",
    "    \n",
    "    try :\n",
    "        # 이미지 생성\n",
    "        result = PIPELINE(\n",
    "            image = images,\n",
    "            prompt = prompt,\n",
    "            negative_prompt = negative_prompt,\n",
    "            height = HEIGHT,\n",
    "            width = WIDTH,\n",
    "            num_images_per_prompt = 4,\n",
    "            num_inference_steps = 20,\n",
    "            strength = 0.7\n",
    "        ).images\n",
    "        \n",
    "    except Exception as e :\n",
    "        print(e)\n",
    "        return e\n",
    "        \n",
    "#     # gpu 메모리 캐시 비우기\n",
    "#     with torch.cuda.device('cuda') :\n",
    "#         torch.cuda.empty_cache()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59dd38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "IMPORTANT: You are using gradio version 3.40.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://bd69caefc8f5a4eb57.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 11 files: 100%|█████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 53153.62it/s]\u001b[A\n",
      "\n",
      "Loading pipeline components...:   0%|                                                             | 0/6 [00:00<?, ?it/s]\u001b[A/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint were not used when initializing CLIPTextModel: \n",
      " ['text_model.embeddings.position_ids']\n",
      "\n",
      "Loading pipeline components...:  50%|██████████████████████████▌                          | 3/6 [00:02<00:02,  1.17it/s]\u001b[A\n",
      "Loading pipeline components...:  67%|███████████████████████████████████▎                 | 4/6 [00:03<00:01,  1.29it/s]\u001b[A\n",
      "Loading pipeline components...:  83%|████████████████████████████████████████████▏        | 5/6 [00:24<00:07,  7.40s/it]\u001b[A\n",
      "Loading pipeline components...: 100%|█████████████████████████████████████████████████████| 6/6 [00:26<00:00,  4.50s/it]\u001b[A\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] initiallized pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:768: FutureWarning: You have passed 4 text prompts (`prompt`), but only 1 initial images (`image`). Initial images are now duplicating to match the number of text prompts. Note that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update your script to pass as many initial images as text prompts to suppress this warning.\n",
      "  deprecate(\"len(prompt) != len(image)\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "\n",
      "  0%|                                                                                            | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|██████                                                                              | 1/14 [00:37<08:10, 37.75s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as app :\n",
    "    \n",
    "    # 모델 다운로드 ui\n",
    "    gr.Markdown('## 모델 다운로드')\n",
    "    with gr.Row() :\n",
    "        model_url = gr.Textbox(label = '모델 URL', placeholder = 'http://civitai.com/')\n",
    "        download_model_btn = gr.Button(value = '모델 다운로드')\n",
    "        \n",
    "    with gr.Row() :\n",
    "        model_file = gr.File(label = '모델 File')\n",
    "        \n",
    "    # 모델 불러오기 ui\n",
    "    gr.Markdown('## 모델 불러오기')\n",
    "    with gr.Row() :\n",
    "        load_model_btn = gr.Button(value = '모델 불러오기')\n",
    "    with gr.Row() :\n",
    "        is_model_check = gr.Textbox(label = 'Model Load Check', value = 'model not loaded')\n",
    "    \n",
    "\n",
    "    # 프롬프트 입력 & 스케치 & 이미지 업로드 ui 구현\n",
    "    gr.Markdown(\"## 프롬프트 입력\")\n",
    "    with gr.Row() :\n",
    "        prompt = gr.Textbox(label = 'Prompt')\n",
    "    with gr.Row() :\n",
    "        n_prompt = gr.Textbox(label = \"negative prompt\")\n",
    "    \n",
    "    # 스케치에서 이미지 생성 블록\n",
    "    gr.Markdown('## 스케치 to 이미지 생성')\n",
    "    with gr.Row() :\n",
    "        with gr.Column() :\n",
    "            with gr.Tab(\"Canvas\") :\n",
    "                with gr.Row() :\n",
    "                    canvas = gr.Image(\n",
    "                        label = 'Draw',\n",
    "                        source = 'canvas',\n",
    "                        image_mode = 'RGB',\n",
    "                        tool = 'color-sketch',\n",
    "                        interactive = True,\n",
    "                        width = WIDTH,\n",
    "                        height = HEIGHT,\n",
    "                        shape = (WIDTH, HEIGHT),\n",
    "                        brush_radius = 20,\n",
    "                        type = 'pil'\n",
    "                    )\n",
    "                with gr.Row() :\n",
    "                    canvas_run_btn = gr.Button(value = 'Generate')\n",
    "\n",
    "            # 파일 업로드\n",
    "            with gr.Tab(\"File\") :\n",
    "                with gr.Row() :\n",
    "                    file = gr.Image(\n",
    "                        label = 'Upload',\n",
    "                        source = 'upload',\n",
    "                        image_mode = 'RGB',\n",
    "                        tool = 'color-sketch',\n",
    "                        interactive = True,\n",
    "                        width = WIDTH,\n",
    "                        height = HEIGHT,\n",
    "                        shape = (WIDTH, HEIGHT),\n",
    "                        type = 'pil'\n",
    "                    )\n",
    "                with gr.Row() :\n",
    "                    file_run_btn = gr.Button(value = 'Generate')\n",
    "                    \n",
    "        # 결과 이미지 갤러리\n",
    "        with gr.Column() :\n",
    "            result_gallery = gr.Gallery(label = 'Output', height = 512)\n",
    "        \n",
    "    # 로드 모델 실행              \n",
    "    load_model_btn.click(\n",
    "        init_pipeline,\n",
    "        None,\n",
    "        [is_model_check]\n",
    "    )\n",
    "    \n",
    "    # 다운로드 모델 실행\n",
    "    download_model_btn.click(\n",
    "        download_model,\n",
    "        [model_url],\n",
    "        [model_file]\n",
    "    )\n",
    "    \n",
    "    # canvas에서 이미지 생성 버튼 실행\n",
    "    canvas_run_btn.click(\n",
    "        sketch_to_image,\n",
    "        [canvas, prompt, n_prompt],\n",
    "        [result_gallery]\n",
    "    )\n",
    "    \n",
    "    # 파일을 업로드했을 때 이미지 생성 버튼 실행\n",
    "    file_run_btn.click(\n",
    "        sketch_to_image,\n",
    "        [file, prompt, n_prompt],\n",
    "        [result_gallery]\n",
    "    )\n",
    "\n",
    "app.queue().launch(inline = False, share = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cdb7c20",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/site-packages/gradio/blocks.py:2175\u001b[0m, in \u001b[0;36mBlocks.close\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver:\n\u001b[0;32m-> 2175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;66;03m# So that the startup events (starting the queue)\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;66;03m# happen the next time the app is launched\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/site-packages/gradio/networking.py:49\u001b[0m, in \u001b[0;36mServer.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d54242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab8b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d152f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f905299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db307e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946796e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c0308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e48f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c06f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca403a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8fe89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaf2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a05146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adc543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b666366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
